{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.Apply Steps of KDD to obtain applicable IPC Section/ BNS Section from the fol/owing complaint, Give useful alrorithms/R/ Python code snippet / processes for identifying applicable BNS (Shartiya Nyay Sanhita) sections\n",
        "### \"*Theft of my mobile phone by an unknown person at MG Road on 2023-02-20. The phone was stolen when I was sopping at a store. The thief was wearing a black jacket and Ind a sear on his face.\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This project processes a text-based crime report and identifies the most applicable sections of IPC (Indian Penal Code) or BNS (Basic National Standards). It involves text preprocessing, entity recognition, relationship extraction, and classification.\n",
        "\n",
        "## Steps for Execution\n",
        "\n",
        "### 1. Text Preprocessing\n",
        "#### Purpose:\n",
        "Prepare the input text for further analysis.\n",
        "\n",
        "#### Sub-steps:\n",
        "1. **Tokenization**:\n",
        "   Break the text into tokens (words or punctuation).\n",
        "2. **Stop Word Removal and Lemmatization**:\n",
        "   Remove commonly used words that do not carry significant meaning (e.g., \"is\", \"the\") and reduce words to their base form.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Entity Recognition\n",
        "#### Purpose:\n",
        "Extract key named entities from the text, such as:\n",
        "- **Crimes**: Theft, assault, fraud, etc.\n",
        "- **Items**: Mobile phone, wallet, etc.\n",
        "- **Locations**: MG Road, Indore, etc.\n",
        "- **Dates**: 2023-02-20, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Pattern or Relationship Extraction\n",
        "#### Purpose:\n",
        "Use dependency parsing to identify relationships between entities (e.g., who did what, where, and when).\n",
        "\n",
        "#### Example:\n",
        "- Identify that \"unknown person\" is linked to \"theft\" at \"MG Road\" on a specific date.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Classification\n",
        "#### Purpose:\n",
        "Map the extracted information to relevant sections of IPC or BNS.\n",
        "\n",
        "#### Example:\n",
        "- Theft might map to **IPC Section 378**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Post-Processing and Ranking\n",
        "#### Purpose:\n",
        "1. Filter and validate the identified sections.\n",
        "2. Rank sections based on relevance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Step 1: Preprocessing**"
      ],
      "metadata": {
        "id": "3wZf8cAtlxU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all', force=True)\n"
      ],
      "metadata": {
        "id": "jSi5qQq3nVjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NRx00Gslsix",
        "outputId": "d1d24dea-975a-4cca-da16-4df3646db651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Theft', 'of', 'my', 'mobile', 'phone', 'by', 'an', 'unknown', 'person', 'at', 'MG', 'Road', 'on', '2023-02-20', '.', 'The', 'phone', 'was', 'stolen', 'when', 'I', 'was', 'shopping', 'at', 'a', 'store', '.', 'The', 'thief', 'was', 'wearing', 'a', 'black', 'jacket', 'and', 'had', 'a', 'scar', 'on', 'his', 'face', '.']\n",
            "Filtered Tokens: ['Theft', 'mobile', 'phone', 'unknown', 'person', 'MG', 'Road', '2023-02-20', '.', 'phone', 'stolen', 'shopping', 'store', '.', 'thief', 'wearing', 'black', 'jacket', 'scar', 'face', '.']\n",
            "Lemmatized Tokens: ['Theft', 'mobile', 'phone', 'unknown', 'person', 'MG', 'Road', '2023-02-20', '.', 'phone', 'stolen', 'shopping', 'store', '.', 'thief', 'wearing', 'black', 'jacket', 'scar', 'face', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the input text\n",
        "text = (\n",
        "    \"Theft of my mobile phone by an unknown person at MG Road on 2023-02-20. \"\n",
        "    \"The phone was stolen when I was shopping at a store. The thief was wearing a black jacket and had a scar on his face.\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    print(\"Filtered Tokens:\", filtered_tokens)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "except LookupError as e:\n",
        "    print(f\"LookupError: {e}\")\n",
        "    print(\"Please ensure the NLTK data is properly installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "PIJ0erJgmHsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Entities:\", entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2wBFO1BmNa2",
        "outputId": "73ddaaa3-c334-41bc-e528-ba07391c6e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: [('MG Road', 'FAC'), ('2023-02-20', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Dependency Parsing**"
      ],
      "metadata": {
        "id": "onpsgGGnmQeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WazBGNmLmaqT",
        "outputId": "7042ba97-e9da-40fc-a7d5-7955597ffcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theft -> ROOT -> Theft\n",
            "of -> prep -> Theft\n",
            "my -> poss -> phone\n",
            "mobile -> amod -> phone\n",
            "phone -> pobj -> of\n",
            "by -> prep -> Theft\n",
            "an -> det -> person\n",
            "unknown -> amod -> person\n",
            "person -> pobj -> by\n",
            "at -> prep -> person\n",
            "MG -> compound -> Road\n",
            "Road -> pobj -> at\n",
            "on -> prep -> person\n",
            "2023 -> nummod -> 20\n",
            "- -> punct -> 2023\n",
            "02 -> appos -> 2023\n",
            "- -> punct -> 20\n",
            "20 -> pobj -> on\n",
            ". -> punct -> Theft\n",
            "The -> det -> phone\n",
            "phone -> nsubjpass -> stolen\n",
            "was -> auxpass -> stolen\n",
            "stolen -> ROOT -> stolen\n",
            "when -> advmod -> shopping\n",
            "I -> nsubj -> shopping\n",
            "was -> aux -> shopping\n",
            "shopping -> advcl -> stolen\n",
            "at -> prep -> shopping\n",
            "a -> det -> store\n",
            "store -> pobj -> at\n",
            ". -> punct -> stolen\n",
            "The -> det -> thief\n",
            "thief -> nsubj -> wearing\n",
            "was -> aux -> wearing\n",
            "wearing -> ROOT -> wearing\n",
            "a -> det -> jacket\n",
            "black -> amod -> jacket\n",
            "jacket -> dobj -> wearing\n",
            "and -> cc -> wearing\n",
            "had -> conj -> wearing\n",
            "a -> det -> scar\n",
            "scar -> dobj -> had\n",
            "on -> prep -> had\n",
            "his -> poss -> face\n",
            "face -> pobj -> on\n",
            ". -> punct -> wearing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Classification**"
      ],
      "metadata": {
        "id": "uSBR1hOnmjJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample mapping dictionary for IPC/BNS sections\n",
        "section_mapping = {\n",
        "    \"theft\": \"IPC 379\",\n",
        "    \"property crime\": \"BNS 4.1\",\n",
        "}\n",
        "\n",
        "# Mapping extracted entities to sections\n",
        "crime = \"theft\"  # Example entity identified\n",
        "section = section_mapping.get(crime, \"Unknown Section\")\n",
        "print(f\"Mapped Section for {crime}: {section}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv6h0D1PmlYO",
        "outputId": "25222435-074f-4316-c943-947ab65932c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped Section for theft: IPC 379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Post-Processing**\n"
      ],
      "metadata": {
        "id": "7HezWJKGmoNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of validation and ranking\n",
        "identified_sections = [(\"IPC 379\", 0.95), (\"BNS 4.1\", 0.90)]\n",
        "validated_sections = sorted(identified_sections, key=lambda x: x[1], reverse=True)\n",
        "print(\"Validated Sections:\", validated_sections)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGU1C2c0mu5U",
        "outputId": "d35a152d-1a05-4423-d0a0-9322925273af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated Sections: [('IPC 379', 0.95), ('BNS 4.1', 0.9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q2. Expion Decision Tree process of data science and its various approaches te construct a decision tree from a database, how these struetures and algorithins be applied to effectively classily tie complaint into appropriate IPC or BNS sections.\n",
        "\n",
        "### Classifying Complaints into IPC/BNS Sections Using Decision Trees\n",
        "\n",
        "The structures and algorithms, such as Decision Trees using Information Gain, can be effectively applied to classify complaints into appropriate IPC or BNS sections by following these steps:\n",
        "\n",
        "### 1. Data Collection and Preparation\n",
        "- Collect and preprocess complaint data, including:\n",
        "  - Tokenization\n",
        "  - Stopword removal\n",
        "  - Entity extraction (e.g., identifying crimes, locations, dates)\n",
        "\n",
        "### 2. Feature Selection\n",
        "- Use attributes from the complaint data (e.g., crime type, location, date, involved parties) as features for classification.\n",
        "\n",
        "### 3. Entropy and Information Gain\n",
        "- Calculate the entropy of the dataset.\n",
        "- Evaluate the information gain for each attribute to determine the best splitting criteria.\n",
        "\n",
        "### 4. Tree Construction\n",
        "- Construct a decision tree by iteratively splitting the data based on the attribute with the highest information gain.\n",
        "\n",
        "### 5. Classification\n",
        "- Traverse the constructed tree to classify each complaint into specific IPC or BNS sections based on the attributes and splitting rules.\n",
        "\n",
        "### 6. Validation and Optimization\n",
        "- Validate the model on test data.\n",
        "- Optimize the model by pruning or adjusting thresholds to improve accuracy.\n",
        "\n",
        "## Conclusion\n",
        "This approach ensures that complaints are classified systematically and aligned with legal frameworks, providing a structured method for handling and categorizing complaint data.\n"
      ],
      "metadata": {
        "id": "IanVH3murQZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. What metries are appropriate for issessing the performance of algorithams used in text, mining? Give Detail the explanation that evaluate the effectiveness and accuracy of text mining techniques."
      ],
      "metadata": {
        "id": "Oa6idOCCswYS"
      }
    }
  ]
}