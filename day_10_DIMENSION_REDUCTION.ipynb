{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geetanshi-jain/DSAssignmentByGeetanshijain/blob/main/day_10_DIMENSION_REDUCTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZciJ_H1Pl_53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ve-Go1o9mptG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZmM8A2dfjLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*#10: DIMENSION REDUCTION\n",
        "\n",
        " Demonstrate How you will  Identify Multicollinearity R/Python\n",
        " Demonstrate HOW you’ll apply PRINCIPAL COMPONENTS ANALYSIS Using R/Python\n",
        "\n",
        "\n",
        "### Dimension Reduction\n",
        "Imagine you have a lot of information about something – like a long list describing a person (height, weight, favorite food, etc.). When there’s too much information, it gets hard to understand and see clear patterns. **Dimension reduction** is about keeping only the most important parts of that information and removing extra stuff to make things simpler.\n",
        "\n",
        "### Multicollinearity\n",
        "Sometimes, some pieces of information overlap a lot. For example, \"height\" and \"shoe size\" might be related (taller people often have bigger feet). This overlap is called **multicollinearity**, and it can make our data confusing and less reliable because it's like repeating the same information in different ways.\n",
        "\n",
        "### How to Find Multicollinearity (Using Python and R)\n",
        "To find out if there’s multicollinearity, we check how much the information overlaps. A common way to do this is with **VIF (Variance Inflation Factor)**. If the VIF value is more than 5 or 10, it means there’s too much overlap, which can be a problem.\n",
        "\n",
        "In Python, it looks like this:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.stats.outliers_influence as inf\n",
        "\n",
        "# Let’s say we have a dataset called 'cereals' with some overlapping information\n",
        "X = pd.DataFrame(cereals[['Sugars', 'Fiber', 'Potass']])\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Checking VIF\n",
        "[vif for vif in [inf.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]]\n",
        "```\n",
        "\n",
        "In R, it would look like this:\n",
        "\n",
        "```R\n",
        "library(car)\n",
        "model <- lm(formula = Rating ~ Fiber + Potass + Sugars, data = cereals)\n",
        "vif(model)\n",
        "```\n",
        "\n",
        "If we get high VIF values, we know we have a problem with too much overlap.\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "When we have too much overlapping information, we can use **PCA (Principal Components Analysis)** to reduce the dimensions. PCA keeps the main parts of the data that are important but removes the overlap to make things simpler.\n",
        "\n",
        "#### How PCA Works\n",
        "1. **Find Main Components**: PCA creates \"components\" – new pieces that keep the main patterns from the original data but without the overlap.\n",
        "2. **Reduce Dimensions**: Instead of keeping all the original data, we only keep a few of these components, which act like summaries of the data.\n",
        "\n",
        "In Python, it would look like this:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Run PCA and keep only 2 main components\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X)\n",
        "```\n",
        "\n",
        "And in R:\n",
        "\n",
        "```R\n",
        "pca <- prcomp(X, center = TRUE, scale = TRUE)\n",
        "summary(pca)\n",
        "```\n",
        "\n",
        "By using PCA, we make our data simpler and easier to understand without all the overlap and confusion.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zSJGEc8fldD"
      }
    }
  ]
}